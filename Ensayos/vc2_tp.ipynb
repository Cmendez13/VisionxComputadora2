{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 75676,
          "sourceType": "datasetVersion",
          "datasetId": 42780
        }
      ],
      "dockerImageVersionId": 30775,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "VC2 TP"
      ],
      "metadata": {
        "id": "Pbfqhzm60aen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import os\n",
        "import cv2 as cv\n",
        "\n",
        "# Load the dataset\n",
        "data_dir = r\"/kaggle/input/natural-images/natural_images\"\n",
        "classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir,d))]\n",
        "print(classes)\n",
        "\n",
        "def dataset(path):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
        "        transforms.RandomHorizontalFlip(0.5),\n",
        "        transforms.RandomVerticalFlip(0.5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "\n",
        "    data_images = torchvision.datasets.ImageFolder(\n",
        "        root=path,\n",
        "        transform=transform\n",
        "    )\n",
        "\n",
        "    train_size = int(0.7*len(data_images))\n",
        "    test_size = len(data_images) - train_size\n",
        "\n",
        "    train_data, test_data = torch.utils.data.random_split(data_images,\n",
        "                                                          [train_size, test_size])\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_data, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        test_data, batch_size=32, shuffle=False, num_workers=4)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Get train and test loaders\n",
        "train_loader, test_loader = dataset(data_dir)\n",
        "\n",
        "# Get a sample of data from the train_loader\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show the dimensions of the images and labels\n",
        "print(f\"Dimensions of the image batch: {images.shape}\")\n",
        "print(f\"Dimensions of the labels: {labels.shape}\")\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        # Definition of convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 512)  # Adjusted for 224x224 input\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Application of convolutional layers\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = self.pool(F.relu(self.conv4(x)))\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs, device=\"cpu\"):\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "        # Evaluate the model after each epoch\n",
        "        evaluate_model(model, test_loader, criterion, device)\n",
        "\n",
        "def evaluate_model(model, test_loader, criterion, device=\"cpu\"):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Validation Loss: {val_loss / len(test_loader):.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Initialize the model, criterion, and optimizer\n",
        "num_classes = len(classes)\n",
        "model = CNN(num_classes)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, test_loader, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-26T01:13:22.850085Z",
          "iopub.execute_input": "2024-09-26T01:13:22.851778Z",
          "iopub.status.idle": "2024-09-26T01:45:19.153943Z",
          "shell.execute_reply.started": "2024-09-26T01:13:22.851685Z",
          "shell.execute_reply": "2024-09-26T01:45:19.151887Z"
        },
        "trusted": true,
        "id": "CU8z3lza0aeq",
        "outputId": "5915a8ad-fb3a-40d5-879e-11fdfb96b38d"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "['motorbike', 'airplane', 'flower', 'dog', 'fruit', 'car', 'cat', 'person']\nDimensions of the image batch: torch.Size([32, 3, 224, 224])\nDimensions of the labels: torch.Size([32])\nEpoch [1/10], Loss: 0.8827\nValidation Loss: 0.5281, Accuracy: 79.76%\nEpoch [2/10], Loss: 0.4747\nValidation Loss: 0.4160, Accuracy: 83.91%\nEpoch [3/10], Loss: 0.3529\nValidation Loss: 0.3327, Accuracy: 87.97%\nEpoch [4/10], Loss: 0.3025\nValidation Loss: 0.3101, Accuracy: 88.50%\nEpoch [5/10], Loss: 0.2608\nValidation Loss: 0.2730, Accuracy: 89.61%\nEpoch [6/10], Loss: 0.2284\nValidation Loss: 0.3169, Accuracy: 88.07%\nEpoch [7/10], Loss: 0.2044\nValidation Loss: 0.2827, Accuracy: 89.08%\nEpoch [8/10], Loss: 0.1770\nValidation Loss: 0.2609, Accuracy: 90.05%\nEpoch [9/10], Loss: 0.1587\nValidation Loss: 0.2706, Accuracy: 90.14%\nEpoch [10/10], Loss: 0.1490\nValidation Loss: 0.2908, Accuracy: 89.13%\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}